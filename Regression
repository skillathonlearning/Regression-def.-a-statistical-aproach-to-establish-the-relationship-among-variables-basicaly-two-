Linear Regression
example, you're building a ML model to identify whether money makes people happy or not? OECD's Better Life Index and GDP per capita

import pandas as pd
import numpy as np
# loading the dataset
oecd_bli = pd.read_csv("../dataset/life-expectancy-GDP/oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv("../dataset/life-expectancy-GDP/gdp_per_capita.csv",thousands=',',delimiter='\t',
                             encoding='latin1', na_values="n/a")
# some data cleaning and preprocessing
def prepare_country_stats(oecd_bli, gdp_per_capita):
    oecd_bli = oecd_bli[oecd_bli["INEQUALITY"]=="TOT"]
    oecd_bli = oecd_bli.pivot(index="Country", columns="Indicator", values="Value")
    gdp_per_capita.rename(columns={"2015": "GDP per capita"}, inplace=True)
    gdp_per_capita.set_index("Country", inplace=True)
    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,
                                  left_index=True, right_index=True)
    full_country_stats.sort_values(by="GDP per capita", inplace=True)
    remove_indices = [0, 1, 6, 8, 33, 34, 35]
    keep_indices = list(set(range(36)) - set(remove_indices))
    return full_country_stats[["GDP per capita", 'Life satisfaction']].iloc[keep_indices]
# Prepare the data
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]
# plot it out
import matplotlib.pyplot as plt

country_stats.plot(kind="scatter", x="GDP per capita", y="Life satisfaction")
plt.show()

general equation for any straight line y = c + mx

life_satisfaction=θ0+θ1∗GDP_per_capita
 
where  θ0  and  θ1  are our ML model's parameters (i.e., Life satisfaction & GDP per capita)

how to measure the performance for this ML model that we'll build?

cost function / utility function / fitness function
for most regression, the cost function is usually the measure of distance b/w the predicted value and the actual value.

various ways to evaluate the performance of a model:

Root Mean Squared Error (RMSE) RMSE basically gives an idea of how much the error your ML system is making in its prediction.

RMSE(X,f)=1m∑i=1m(f(xi)−yi)2−−−−−−−−−−−−−−−√
 
where m : total no. of instances
f: hypothesis function/Model prediction function
f(x): predicted value after going through prediction function
y: actual target value
xi :  ith  element of the feature vector x

Mean Squared Error (MSE) practically similar to RMSE except for the sqaure root

Mean Absolute Error (MAE) gives you the same idea of how much your predicted values is deviating from the actual values.

The only difference is in the way the distances are calculated in RMSE/MSE and MAE

MAE(X,f)=1m∑i=1m|f(xi)−yi)|
 
RMSE basically uses Euclidean distance approach. Also called as l2 norm of measuring distances.
MAE uses Manhattan distance approach. Also called as l1 norms of measuring distances.

there are other higher order norms of measuring these distances and as we go along increasing the norm (something like  lk  norm ) these functions start focusing more on larger values and kind of start ignoring the smaller values

therefore, RMSE is more sensitive to outliers than MAE

[NOTE] when we have exponentially less outliers, then even RMSE performs well.

# training our model

from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(X, y)
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
         normalize=False)
# to get intercept value
t0 = model.intercept_
t0
array([4.8530528])
# to get coefficient values
t1 = model.coef_
t1
array([[4.91154459e-05]])
so for our case, the most optimal parameter would be  θ0=4.853  and  θ1=4.911x10−5 
# predicting life satisfaction given some GDP pe capita value

some_gdp = [[1717]] # GDP per capita of India in 2016
model.predict(some_gdp)
array([[4.93738402]])
The Normal equation
to find the best value for out  θ  that would minimize the cost function to its least, we use a mathematical approach called The Normal Equation

θ^=(XT.X)−1.XT.y
 
where  θ^ : value that minimizes the cost function
X: feature vector
y: target value

# lets generate some random linear dataset

X = 2 * np.random.rand(100,1)
y = 4 + 3 * X + np.random.rand(100, 1)   
this is basicall,

y=4+3x0+someNoise
 
plt.plot(X, y, "b")
plt.xlabel("$x_1$", fontsize=12)
plt.ylabel("$y$", fontsize=12)
plt.axis([0, 2, 0, 15])

plt.show()

X_b = np.c_[np.ones((100, 1)), X] # adding x0 = 1 to each instance
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) # normal eq.
theta_best
array([[4.56144209],
       [2.97500257]])
X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new] # adding x0=1 bias to each instance

y_predict = X_new_b.dot(theta_best)
y_predict
array([[ 4.56144209],
       [10.51144722]])
plt.figure(figsize=(8,6))
plt.plot(X_new, y_predict, "r-")
plt.plot(X, y, "b.")
plt.axis([0, 2, 0, 15])
plt.show()

Gradient Descent:
Gradient Descent is basically generic optimizatio algorithm capable of finding the optimal solution to wide range of problems.

[story inside it] the general idea of GD is to tweak your parameters of a model iteratively so as to minimize its cost function.

remember the mountain story [core idea]

what values shoud  θ  have initially? well we start out filling int random values for our model parameters. this is also called random initialization

remember the baby steps the man takes to descent down the hill thats what we call learning rate

if your learning rate is too small, your model will have to go through many iterations to converge to global minima wherein your cost function is at its least

on the other hand, if you set it out to be a large value, chances are that you jump across the valleys, you might even end up at a point thats higher than the value you had initially, this again will more time to train

ideally people experiment out with diff diff values for the learning rate.

BONUS the MSE is originally a convex function in nature.

TIP if you scale your feature vectors to the same scale, you have chances you reaching the minima for your cost function at the eariest.

Feature Scaling
[core idea] whenever you have numerical attributes all in diff diff ranges (e.g. 10% - 54%, 0.12 to 0.99. 28 to 34324),
you would want to bring all these numerical attributes onto the same scale. That's when you do feature scaling.

two most common approaches are:

Min-Max Scaling : you shift every numerical attirbute b/w a min and a max range.
Normalization/ Standard Scaling : you subtract every numerical attirbute with its minimum and then yu divide it out unit variance.
Zmin−maxscalling=x−min(x)(max(x)−min(x))
 
from sklearn.preprocessing import MinMaxScaler
Znormalization=X−μσ
 
from sklearn.preprocessing import StandardScaler
where  μ  is mean
X: feature vector
σ  is standard deviation

Batch Gradient Descent:
to implement any GD, we first calculate the gradient of the cost function w.r.t the model parameters  θi 
in other words how much will our cost func change in its magnitude when we slightly change the values  θi .

this is actually achieved through partial derivation of the changes in cost function

δδθjMSE(θ)=2m∑i=1m(θT.xi−yi).xij
 
gradient vector of the cost function

▽θMSE(θ)=2mXT.(X.θ−y)
 
this will basically point you to the uphill, but our goal is to descend or reach the minima, therefore, we'll go in the opposite direction of the uphill. Hence, the formula for next gradient descent step will be:

θnextstep=θ−α▽θMSE(θ)
 
where  α▽θMSE(θ)  is your size of the step
α  is learning rate

Stochastic Gradient Descent
[core idea] the problem with BGD, is that it uses the whole training set to compute the gradient at each step, which makes it very slow to train on larger dataset.
SGD just picks some random instance in the training set and then computes the gradient of the cost function at that single instance. so less comutation in tweaking our model parameters (bcz this is a tweak just across one instance & not the entire parameters of the entire dataset). Hence its faster at reaching you global minima!

stochastic means random.

SGD more preferred ove BGD

Mini Batch Gradient Descent
it just grabs a smalled sub-set of the entire dataset (called batch) and then computes the gradient descent for the cost function for this whole batch before moving ahead.

 
Polynomial Regression
y=ax2+bx+c
 
(2 degree polynomial function)

[core idea] whenever there's non-linear dataset, you can't straight away fit in a simple linear rule. You then imply a higher order polynomial function

np.random.seed(29)

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.6 * X**2 + X * 2 + np.random.randn(m, 1) # quadratic equation
plt.plot(X, y, "b.")
plt.xlabel("$X_1$", fontsize=12)
plt.ylabel("$y$", fontsize=12)
plt.axis([-7, 7, -5, 10])
plt.show()

from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2, include_bias=False)

X_poly = poly_features.fit_transform(X)
X[0]
array([2.18255991])
X_poly[0]
array([2.18255991, 4.76356778])
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
(array([-0.13563025]), array([[1.92097687, 0.64456286]]))
# plot it out

X_new = np.linspace(-7, 7, 100).reshape(100,1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)

plt.plot(X, y, "b.")
plt.plot(X_new, y_new, "r-", label="Prediction")
plt.xlabel("$x1$", fontsize=12)
plt.ylabel("$y$", fontsize=12)
plt.axis([-7, 7, -5, 10])
plt.legend(loc="lower right", fontsize=12)

plt.show()

# lets try SGD Regressor now

from sklearn.linear_model import SGDRegressor

sgd_rg = SGDRegressor(random_state=29)
sgd_rg.fit(X_poly, y)
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.
  FutureWarning)
C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,
       eta0=0.01, fit_intercept=True, l1_ratio=0.15,
       learning_rate='invscaling', loss='squared_loss', max_iter=None,
       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,
       random_state=29, shuffle=True, tol=None, validation_fraction=0.1,
       verbose=0, warm_start=False)
sgd_rg.intercept_, sgd_rg.coef_
(array([0.0040066]), array([1.88374482, 0.60669213]))
